{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d997778b-c1c1-4121-b66f-b457f0f3ea09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T06:24:26.273667Z",
     "iopub.status.busy": "2024-08-22T06:24:26.273417Z",
     "iopub.status.idle": "2024-08-22T06:24:26.380623Z",
     "shell.execute_reply": "2024-08-22T06:24:26.380049Z",
     "shell.execute_reply.started": "2024-08-22T06:24:26.273649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main program\n",
      "Using device: cuda\n",
      "Loading dataset\n",
      "Initializing JetDataset with file: ../data/val/val_20_30.h5\n",
      "Error loading dataset: [Errno 2] Unable to synchronously open file (unable to open file: name = '../data/val/val_20_30.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Creating DataLoader\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 513\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating DataLoader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 513\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m model \u001b[38;5;241m=\u001b[39m JJEPA(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import h5py\n",
    "\n",
    "def check_raw_data(subjets_data, jet_index=0):\n",
    "    print(f\"\\n--- Checking Raw Data for Jet {jet_index} ---\")\n",
    "    print(f\"Number of subjets: {subjets_data['subjet_pt'].shape[-1]}\")\n",
    "    print(f\"Subjet features: {list(subjets_data.keys())}\")\n",
    "    print(f\"Number of indices per subjet: {subjets_data['particle_indices'].shape[-1]}\")\n",
    "    print(f\"Sample subjet feature values: \")\n",
    "    for name in subjets_data.keys():\n",
    "        print(name, subjets_data[name][jet_index])\n",
    "\n",
    "def check_model_input(model_input, batch_index=0):\n",
    "    print(f\"\\n--- Checking Model Input for Batch Item {batch_index} ---\")\n",
    "    print(f\"Input shape: {model_input.shape}\")\n",
    "    if len(model_input.shape) == 3:\n",
    "        batch_size, num_subjets, feature_dim = model_input.shape\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Number of subjets: {num_subjets}\")\n",
    "        print(f\"Feature dimension: {feature_dim}\")\n",
    "        print(\"\\nFirst few values of the first subjet:\")\n",
    "        print(model_input[0, 0, :10])\n",
    "    else:\n",
    "        print(\"Unexpected shape for model input\")\n",
    "\n",
    "def inspect_indices(subjets, num_samples=5):\n",
    "    print(\"\\n--- Inspecting Subjet Indices ---\")\n",
    "    for i in range(min(num_samples, len(subjets['subjet_eta']))):\n",
    "        print(f\"\\nSubjet {i}:\")\n",
    "        print(f\"  pT: {subjets['subjet_pt'][i]:.2f}\")\n",
    "        print(f\"  eta: {subjets['subjet_eta'][i]:.2f}\")\n",
    "        print(f\"  phi: {subjets['subjet_phi'][i]:.2f}\")\n",
    "        print(f\"  num_ptcls: {subjets['subjet_num_ptcls'][i]}\")\n",
    "        print(f\"  Indices: {subjets['particle_indices'][i, :10]}...\")  # Print first 10 indices\n",
    "        print(f\"  Number of indices: {len(subjets['particle_indices'][i])}\")\n",
    "\n",
    "def normalize_features(particle_features, feature_names, config):\n",
    "    print(f\"Normalizing particle features with shape: {particle_features.shape}\")\n",
    "    \n",
    "    normalized_features = particle_features.clone()\n",
    "    \n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        method = config[\"INPUTS\"][\"SEQUENTIAL\"][\"Particles\"].get(feature_name, \"none\")\n",
    "        print(f\"Normalizing particle feature '{feature_name}' with method '{method}'\")\n",
    "        \n",
    "        if method == \"normalize\":\n",
    "            mean = particle_features[:, i].mean()\n",
    "            std = particle_features[:, i].std()\n",
    "            std = std if std > 1e-8 else 1.0\n",
    "            normalized_features[:, i] = (particle_features[:, i] - mean) / std\n",
    "        elif method == \"log_normalize\":\n",
    "            normalized_features[:, i] = torch.log1p(particle_features[:, i])\n",
    "      \n",
    "    \n",
    "    print(f\"Normalized particle features shape: {normalized_features.shape}\")\n",
    "    \n",
    "    return normalized_features\n",
    "\n",
    "class DimensionCheckLayer(torch.nn.Module):\n",
    "    def __init__(self, name, expected_dims):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.expected_dims = expected_dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) != self.expected_dims:\n",
    "            print(f\"WARNING: {self.name} has {len(x.shape)} dimensions, expected {self.expected_dims}\")\n",
    "        return x\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    \"\"\"JetDataset class for loading and processing jet data from HDF5 files.\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, num_jets=None, transform=None, config=None, debug=False):\n",
    "        print(f\"Initializing JetDataset with file: {file_path}\")\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            print(\"Loading features and subjets from HDF5 file\")\n",
    "            self.particles = {name: hdf['particles'][name][:] for name in hdf['particles']}\n",
    "            self.subjets = {name: hdf['subjets'][name][:] for name in hdf['subjets']}\n",
    "            self.labels = hdf['labels'][:]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.config = config\n",
    "        self.debug = debug\n",
    "        if self.debug:\n",
    "            print(f\"Raw dataset size: {self.labels.shape[0]} jets\")\n",
    "            print(\"Particle features shapes\")\n",
    "            for name in self.particles.keys():\n",
    "                print(f\"shape of {name}: {self.particles[name].shape}\")\n",
    "            print(\"Subjet features shapes\")\n",
    "            for name in self.subjets.keys():\n",
    "                print(f\"shape of {name}: {self.subjets[name].shape}\")\n",
    "        \n",
    "        self.filter_good_jets()\n",
    "        \n",
    "        if num_jets is not None:\n",
    "            if self.debug:\n",
    "                print(f\"Reducing number of jets to: {num_jets}\")\n",
    "            for name in self.particles.keys():\n",
    "                self.particles[name] = self.particles[name][:num_jets]\n",
    "                if self.debug:\n",
    "                    print(f\"shape of {name}: {self.particles[name].shape}\")\n",
    "            for name in self.subjets.keys():\n",
    "                self.subjets[name] = self.subjets[name][:num_jets]\n",
    "                if self.debug:\n",
    "                    print(f\"shape of {name}: {self.subjets[name].shape}\")\n",
    "        self.labels = self.labels[:num_jets]\n",
    "        print(f\"Final dataset size: {self.labels.shape[0]} jets\")\n",
    "\n",
    "    def filter_good_jets(self):\n",
    "        \"\"\"\n",
    "        Filters jets based on the number of real subjets and updates the dataset to only include 'good' jets.\n",
    "        \n",
    "        Modifies:\n",
    "        - Updates self.particles, self.subjets, and self.labels to only include jets with at least 10 real subjets.\n",
    "        \n",
    "        Prints:\n",
    "        - The shape of particles and subjets after filtering.\n",
    "        - The total number of good jets remaining after filtering.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"Filtering good jets...\")\n",
    "        good_jet_indices = []\n",
    "        \n",
    "        for jet_idx in range(self.labels.shape[0]):\n",
    "            num_real_subjets = self.get_num_real_subjets(jet_idx)\n",
    "            # print(f\"Jet {jet_idx}: {num_real_subjets} real subjets\")\n",
    "            if num_real_subjets >= 10:\n",
    "                good_jet_indices.append(jet_idx)\n",
    "        \n",
    "        for name in self.particles.keys():\n",
    "            self.particles[name] = self.particles[name][good_jet_indices]\n",
    "            if self.debug:\n",
    "                print(f\"shape of {name}: {self.particles[name].shape}\")\n",
    "        for name in self.subjets.keys():\n",
    "            self.subjets[name] = self.subjets[name][good_jet_indices]\n",
    "            if self.debug:\n",
    "                print(f\"shape of {name}: {self.subjets[name].shape}\")\n",
    "        self.labels = self.labels[good_jet_indices]\n",
    "        print(f\"Filtered to {self.labels.shape[0]} good jets\")\n",
    "    \n",
    "    def get_num_real_subjets(self, jet_idx):\n",
    "        \"\"\"\n",
    "        Returns the number of real subjets in a given jet.\n",
    "        Parameters:\n",
    "        - jet_idx: the index of the jet to check\n",
    "        Returns:\n",
    "        - int: The number of real subjets.\n",
    "        Example:\n",
    "        >>> self.subjets['subjet_num_ptcls'][jet_idx] = [15, 10, 10, 9, 8, 0, 0, 0, ..., 0]\n",
    "        >>> get_num_real_subjets(jet_idx)\n",
    "        5\n",
    "        \"\"\"\n",
    "        return sum(1 for subjet_num_ptcls in self.subjets['subjet_num_ptcls'][jet_idx] if subjet_num_ptcls > 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the jets in the dataset.\n",
    "        Returns:\n",
    "            int: Number of jets in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrievs the features and subjets for a given index and processes them.\n",
    "        Args:\n",
    "            idx (int): The index of the item to fetch.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following elements:\n",
    "                - features (numpy.ndarray): The normalized particle features of the item.\n",
    "                - subjets (numpy.ndarray): The subjets data of the item.\n",
    "                - subjet_mask (numpy.ndarray): The subjet mask data of the item.\n",
    "                - particle_mask (numpy.ndarray): The particle mask data of the item.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            check_raw_data(self.subjets, jet_index=idx)  # -- Debug statment\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"\\nFetching item {idx} from dataset\")\n",
    "        particle_feature_names = ['part_deta', 'part_dphi', 'part_pt_log', 'part_e_log']\n",
    "        particle_features = np.stack([self.particles[name][idx] for name in particle_feature_names])\n",
    "        if self.debug:\n",
    "            print(\"particle features shape\", particle_features.shape)\n",
    "        subjets = {name: self.subjets[name][idx] for name in self.subjets.keys()}\n",
    "        if self.debug:\n",
    "            print(\"subjets\", subjets)\n",
    "\n",
    "        if self.debug:\n",
    "            inspect_indices(subjets) # -- Debug statment\n",
    "\n",
    "        subjets, indices, subjet_mask, particle_mask = self.process_subjets(subjets)\n",
    "\n",
    "        print(\"Normalizing features\")\n",
    "        particle_features = torch.from_numpy(particle_features)\n",
    "        if self.config:\n",
    "            particle_features = normalize_features(particle_features, particle_feature_names, self.config)\n",
    "        else:\n",
    "            print(\"No config provided for normalization\")\n",
    "        \n",
    "        if self.transform:\n",
    "            print(\"Applying transform to features\")\n",
    "            features = self.transform(features)\n",
    "        \n",
    "        # print(f\"Returning data for item {idx}\")\n",
    "        # print(f\"Features shape: {features.shape}\")\n",
    "        # print(f\"Subjets shape: {subjets.shape}\")\n",
    "        # print(f\"Subjet mask shape: {subjet_mask.shape}\")\n",
    "        # print(f\"Particle mask shape: {particle_mask.shape}\")\n",
    "        # print(f\"Subjets data: {subjets}\")\n",
    "        # print(f\"Subjet mask data: {subjet_mask}\")\n",
    "        # print(f\"Particle mask data: {particle_mask}\")\n",
    "\n",
    "        return particle_features, subjets, indices, subjet_mask, particle_mask\n",
    "\n",
    "    def process_subjets(self, subjets):\n",
    "        \"\"\"\n",
    "        Processes subjets to create tensor representations and masks.\n",
    "\n",
    "        Parameters:\n",
    "        - subjets (dictionary): a dictionary with keys ['subjet_pt', 'subjet_eta', 'subjet_phi', 'subjet_num_ptcls', 'particle_indices']. \n",
    "            each item is a numpy array of shape (N_subjets, ) or (N_subjets, N_particles) for the last key\n",
    "\n",
    "        Returns:\n",
    "        - tuple: (subjets, subjet_mask, particle_mask)\n",
    "            where `subjets` is the tensor representation of subjets,\n",
    "            `subjet_mask` is the mask for subjets,\n",
    "            and `particle_mask` is the mask for particles.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"Processing subjets\")\n",
    "\n",
    "        max_len = subjets['particle_indices'].shape[-1]\n",
    "        if self.debug:\n",
    "            print(f\"Max length of indices in subjets: {max_len}\")\n",
    "        subjet_mask = torch.tensor(subjets['subjet_num_ptcls'] != 0, dtype=torch.float32)\n",
    "        \n",
    "        particle_mask = torch.tensor(subjets['particle_indices'] != -1, dtype=torch.float32)\n",
    "\n",
    "        features = torch.stack([torch.from_numpy(subjets[name]) for name in ['subjet_pt', 'subjet_eta', 'subjet_phi', 'subjet_num_ptcls']], dim=0)\n",
    "        features = features.t()\n",
    "        \n",
    "        indices = torch.from_numpy(subjets['particle_indices'])\n",
    "        if self.debug:\n",
    "            print(f\"Subjet indices shape: {indices.shape}\")  # (N_subjets, N_particles)\n",
    "            \n",
    "            print(f\"Final processed subjets shape: {features.shape}\") # (N_subjets, 4)\n",
    "            print(f\"Final subjet mask shape: {subjet_mask.shape}\") # (N_subjets, ) same as 'subjet_num_ptcls'\n",
    "            print(f\"Final particle mask shape: {particle_mask.shape}\") # (N_subjets, N_particles) same as 'particle_indices'\n",
    "        \n",
    "        return features, indices, subjet_mask, particle_mask\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        print(\"Initializing Attention module\")\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Attention forward pass with input shape: {x.shape}\")\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        print(f\"Attention output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        print(\"Initializing MLP module\")\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"MLP forward pass with input shape: {x.shape}\")\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        print(f\"MLP output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing Block module\")\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = nn.Identity() if drop_path <= 0 else nn.Dropout(drop_path)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Block forward pass with input shape: {x.shape}\")\n",
    "        y = self.attn(self.norm1(x))\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        print(f\"Block output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class JetsTransformer(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim, depth, num_heads, mlp_ratio, qkv_bias=False, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing JetsTransformer module\")\n",
    "        self.num_features = num_features\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Adjust the input dimensions based on the new input shape\n",
    "        self.patch_embed = nn.Linear(num_features * 30, embed_dim)  # num_features * subjet_length\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 512, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate, norm_layer=norm_layer) for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"JetsTransformer forward pass with input shape: {x.shape}\")\n",
    "        B, N, C, L = x.shape\n",
    "        x = x.view(B, N, -1)  # Flatten last two dimensions to [B, N, C*L]\n",
    "        print(f\"Flattened input shape: {x.shape}\")\n",
    "        x = self.patch_embed(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        print(f\"JetsTransformer output shape: {x.shape}\")\n",
    "        return x.view(B, N, -1)  # Reshape back if necessary\n",
    "\n",
    "class JetsTransformerPredictor(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim, predictor_embed_dim, depth, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing JetsTransformerPredictor module\")\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate, norm_layer=norm_layer) for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, 8 * 30, bias=True)  # Match target dimensions\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        print(f\"JetsTransformerPredictor forward pass with input shape: {x.shape}\")\n",
    "        x = self.predictor_embed(x)\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_proj(x)\n",
    "        print(f\"JetsTransformerPredictor output shape: {x.shape}\")\n",
    "        return x.view(x.size(0), x.size(1), 8, 30)  # Reshape to match target_repr shape\n",
    "\n",
    "class JJEPA(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, depth, num_heads, mlp_ratio, dropout=0.1, use_predictor=True):\n",
    "        super(JJEPA, self).__init__()\n",
    "        print(\"Initializing JJEPA module\")\n",
    "        self.use_predictor = use_predictor\n",
    "        self.context_transformer = JetsTransformer(num_features=input_dim, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=dropout)\n",
    "        if self.use_predictor:\n",
    "            self.predictor_transformer = JetsTransformerPredictor(num_features=input_dim, embed_dim=embed_dim, predictor_embed_dim=embed_dim//2, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=dropout)\n",
    "\n",
    "\n",
    "        # Debug Statement - Dimension check\n",
    "        self.input_check = DimensionCheckLayer(\"Model Input\", 3)\n",
    "        self.context_check = DimensionCheckLayer(\"After Context Transformer\", 3)\n",
    "        self.predictor_check = DimensionCheckLayer(\"After Predictor\", 3)\n",
    "\n",
    "    def forward(self, context, target):\n",
    "        print(f\"JJEPA forward pass with context shape: {context.shape} and target shape: {target.shape}\")\n",
    "        context = context.to(next(self.parameters()).device)\n",
    "        target = target.to(next(self.parameters()).device)\n",
    "        \n",
    "        context_repr = self.context_transformer(context)\n",
    "        # Debug Statement\n",
    "        context_repr = self.context_check(context_repr)\n",
    "        if self.use_predictor:\n",
    "            pred_repr = self.predictor_transformer(context_repr, None, None)\n",
    "            pred_repr = self.predictor_check(pred_repr)\n",
    "            print(f\"JJEPA output - pred_repr shape: {pred_repr.shape}, context_repr shape: {context_repr.shape}, target shape: {target.shape}\")\n",
    "            return pred_repr, context_repr, target\n",
    "        \n",
    "        print(f\"JJEPA output - context_repr shape: {context_repr.shape}, target shape: {target.shape}\")\n",
    "        return context_repr, target\n",
    "\n",
    "def train_step(model, subjets, subjet_masks, particle_masks, partioptimizer, device, step):\n",
    "    print(f\"\\nStarting training step {step}\")\n",
    "    \n",
    "    batch_size, num_subjets, num_features, subjet_length = subjets.size()\n",
    "    print(f\"Input shapes - Subjets: {subjets.shape}, Subjet masks: {subjet_masks.shape}, Particle masks: {particle_masks.shape}\")\n",
    "    \n",
    "    context_masks, target_masks = create_random_masks(batch_size, num_subjets, num_features, subjet_length)\n",
    "    print(f\"Context masks shape: {context_masks.shape}, Target masks shape: {target_masks.shape}\")\n",
    "    \n",
    "    context_masks = context_masks.to(device)\n",
    "    target_masks = target_masks.to(device)\n",
    "    subjet_masks = subjet_masks.to(device)\n",
    "    particle_masks = particle_masks.to(device)\n",
    "    \n",
    "    context_subjets = subjets * context_masks\n",
    "    target_subjets = subjets * target_masks\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Forwarding through model\")\n",
    "    pred_repr, context_repr, target_repr = model(context_subjets, target_subjets)\n",
    "    \n",
    "    print(f\"Predicted representation shape: {pred_repr.shape}\")\n",
    "    print(f\"Target representation shape: {target_repr.shape}\")\n",
    "    \n",
    "    combined_mask = target_masks.to(device) * subjet_masks.unsqueeze(-1).unsqueeze(-1).expand_as(target_masks).to(device)\n",
    "    \n",
    "    pred_repr = pred_repr.to(device)\n",
    "    target_repr = target_repr.to(device)\n",
    "    \n",
    "    print(\"Calculating loss\")\n",
    "    loss = F.mse_loss(pred_repr * combined_mask, target_repr * combined_mask)\n",
    "    print(f\"Calculated loss: {loss.item()}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print_jet_details(pred_repr[0].cpu(), \"Predicted\")\n",
    "        print(f\"Context representation shape: {context_repr.shape}\")\n",
    "        print(f\"Target representation shape: {target_repr.shape}\")\n",
    "        \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def create_random_masks(batch_size, num_subjets, num_features, subjet_length, context_scale=0.7):\n",
    "    print(f\"Creating random masks with batch_size={batch_size}, num_subjets={num_subjets}\")\n",
    "    context_masks = []\n",
    "    target_masks = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        indices = torch.randperm(num_subjets)\n",
    "        context_size = int(num_subjets * context_scale)\n",
    "        context_indices = indices[:context_size]\n",
    "        target_indices = indices[context_size:]\n",
    "\n",
    "        context_mask = torch.zeros(num_subjets, num_features, subjet_length)\n",
    "        target_mask = torch.zeros(num_subjets, num_features, subjet_length)\n",
    "\n",
    "        context_mask[context_indices] = 1\n",
    "        target_mask[target_indices] = 1\n",
    "\n",
    "        context_masks.append(context_mask)\n",
    "        target_masks.append(target_mask)\n",
    "\n",
    "    return torch.stack(context_masks), torch.stack(target_masks)\n",
    "\n",
    "def print_jet_details(jet, name):\n",
    "    print(f\"\\n{name} Jet Details:\")\n",
    "    print(f\"Shape: {jet.shape}\")\n",
    "    print(f\"Non-zero elements: {torch.count_nonzero(jet)}\")\n",
    "    print(\"\\nFirst few elements:\")\n",
    "    print(jet[0, :5, :5])\n",
    "\n",
    "\n",
    "def visualize_training_loss(train_losses):\n",
    "    print(\"Visualizing training loss\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting main program\")\n",
    "    \n",
    "    current_dir = \"/ssl-jet-vol-v3/I-JEPA-Jets-Subash/notebooks\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    try:\n",
    "        os.chdir(os.path.join(os.getcwd(), '..', '..'))\n",
    "        os.chdir('I-JEPA-Jets/')\n",
    "        print(\"Loading dataset\")\n",
    "        # train_dataset = JetDataset(\"../data/val/val_20_30.h5\", num_jets=1000, config=config)\n",
    "        train_dataset = JetDataset(\"data/val/val_20_30_new.h5\", num_jets=1000, config=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "    print(\"Creating DataLoader\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    print(\"Initializing model\")\n",
    "    model = JJEPA(input_dim=240, embed_dim=512, depth=12, num_heads=8, mlp_ratio=4.0, dropout=0.1).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.04)\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True, position=0)\n",
    "    \n",
    "        for step, (features, subjets, subjet_masks, particle_masks) in enumerate(train_loader):\n",
    "            features = features.to(device)\n",
    "            subjets = subjets.to(device)\n",
    "            subjet_masks = subjet_masks.to(device)\n",
    "            particle_masks = particle_masks.to(device)\n",
    "            \n",
    "            loss = train_step(model, subjets, subjet_masks, particle_masks, optimizer, device, step)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss)\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"\\nEpoch {epoch+1}, Step {step}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        progress_bar.close()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    print(\"Visualizing training loss\")\n",
    "    visualize_training_loss(train_losses)\n",
    "\n",
    "    print(\"Saving model\")\n",
    "    torch.save(model.state_dict(), 'ijepa_model.pth')\n",
    "\n",
    "    print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c5f8f-f921-4bb9-99be-b6dea7aa7c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
