{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d997778b-c1c1-4121-b66f-b457f0f3ea09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T06:24:26.273667Z",
     "iopub.status.busy": "2024-08-22T06:24:26.273417Z",
     "iopub.status.idle": "2024-08-22T06:24:26.380623Z",
     "shell.execute_reply": "2024-08-22T06:24:26.380049Z",
     "shell.execute_reply.started": "2024-08-22T06:24:26.273649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main program\n",
      "Using device: cuda\n",
      "Loading dataset\n",
      "Initializing JetDataset with file: ../data/val/val_20_30.h5\n",
      "Error loading dataset: [Errno 2] Unable to synchronously open file (unable to open file: name = '../data/val/val_20_30.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Creating DataLoader\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 513\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating DataLoader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 513\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m model \u001b[38;5;241m=\u001b[39m JJEPA(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import h5py\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    \"\"\"JetDataset class for loading and processing jet data from HDF5 files.\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, subset_size=None, transform=None, config=None):\n",
    "        print(f\"Initializing JetDataset with file: {file_path}\")\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            print(\"Loading features and subjets from HDF5 file\")\n",
    "            self.features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "            self.subjets = [json.loads(subjet) for subjet in hdf[\"subjets\"][:]]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.config = config\n",
    "        print(f\"Raw dataset size: {len(self.subjets)} jets\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        \n",
    "        self.filter_good_jets()\n",
    "        \n",
    "        if subset_size is not None:\n",
    "            print(f\"Applying subset size: {subset_size}\")\n",
    "            self.features = self.features[:subset_size]\n",
    "            self.subjets = self.subjets[:subset_size]\n",
    "        \n",
    "        print(f\"Final dataset size: {len(self.subjets)} jets\")\n",
    "\n",
    "    def filter_good_jets(self):\n",
    "        \"\"\"\n",
    "        Filters jets to retain only those with a sufficient number of real subjets.\n",
    "        \"\"\"\n",
    "        print(\"Filtering good jets...\")\n",
    "        good_jets = []\n",
    "        good_features = []\n",
    "        \n",
    "        for i in range(len(self.subjets)):\n",
    "            num_real_subjets = self.get_num_real_subjets(self.subjets[i])\n",
    "            if num_real_subjets >= 10:\n",
    "                good_jets.append(self.subjets[i])\n",
    "                good_features.append(self.features[i])\n",
    "        \n",
    "        self.subjets = good_jets\n",
    "        self.features = torch.stack(good_features)\n",
    "        print(f\"Filtered to {len(self.subjets)} good jets\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_num_real_subjets(jet):\n",
    "        \"\"\"\n",
    "        Returns the number of real subjets in a given jet.\n",
    "        \"\"\"\n",
    "        return sum(1 for subjet in jet if subjet['features']['num_ptcls'] > 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the features and subjets for a given index and processes them.\n",
    "        \"\"\"\n",
    "        check_raw_data(self.subjets, jet_index=idx)  # Debug statement\n",
    "\n",
    "        print(f\"\\nFetching item {idx} from dataset\")\n",
    "        features = self.features[idx]\n",
    "        subjets = self.subjets[idx]\n",
    "\n",
    "        inspect_subjets(subjets)  # Debug statement\n",
    "\n",
    "        subjets, subjet_mask, particle_mask = self.process_subjets(subjets)\n",
    "        \n",
    "        check_processed_data(subjets)  # Debug statement\n",
    "\n",
    "        feature_names = ['pT', 'eta', 'phi']\n",
    "        print(\"Normalizing features\")\n",
    "        features = normalize_features(features, feature_names, self.config, jet_type='Jets')\n",
    "        \n",
    "        if self.transform:\n",
    "            print(\"Applying transform to features\")\n",
    "            features = self.transform(features)\n",
    "        \n",
    "        return features, subjets, subjet_mask, particle_mask\n",
    "\n",
    "    def process_subjets(self, subjets):\n",
    "        \"\"\"\n",
    "        Processes subjets to create tensor representations and masks.\n",
    "        \"\"\"\n",
    "        print(\"Processing subjets\")\n",
    "\n",
    "        max_len = max(len(subjet['indices']) for subjet in subjets)\n",
    "        print(f\"Max length of indices in subjets: {max_len}\")\n",
    "        subjet_tensors = []\n",
    "        subjet_mask = []\n",
    "        particle_mask = []\n",
    "        \n",
    "        for i, subjet in enumerate(subjets):\n",
    "            feature_tensors = [torch.tensor([subjet['features'][k]], dtype=torch.float32).expand(max_len) for k in ['pT', 'eta', 'phi', 'num_ptcls']]\n",
    "            features = torch.stack(feature_tensors, dim=0)\n",
    "            \n",
    "            is_empty = subjet['features']['num_ptcls'] == 0\n",
    "            subjet_mask.append(0 if is_empty else 1)\n",
    "            particle_mask.append([1 if i < len(subjet['indices']) else 0 for i in range(max_len)])\n",
    "\n",
    "            subjet_tensors.append(features)\n",
    "\n",
    "        subjets = torch.stack(subjet_tensors)\n",
    "        subjet_mask = torch.tensor(subjet_mask, dtype=torch.float32)\n",
    "        particle_mask = torch.tensor(particle_mask, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Final processed subjets shape: {subjets.shape}\")\n",
    "        print(f\"Final subjet mask shape: {subjet_mask.shape}\")\n",
    "        print(f\"Final particle mask shape: {particle_mask.shape}\")\n",
    "        \n",
    "        return subjets, subjet_mask, particle_mask\n",
    "\n",
    "def custom_collate_fn(batch):    \n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader to handle variable-sized subjets and particles.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting custom_collate_fn ---\")\n",
    "    \n",
    "    features, subjets, subjet_masks, particle_masks = zip(*batch)\n",
    "    \n",
    "    features = torch.stack(features)\n",
    "    print(f\"Features shape after stacking: {features.shape}\")\n",
    "    \n",
    "    max_subjets = max(s.size(0) for s in subjets)\n",
    "    max_subjet_features = max(s.size(1) for s in subjets)\n",
    "    max_subjet_length = max(s.size(2) for s in subjets)\n",
    "    \n",
    "    print(f\"Max subjets: {max_subjets}\")\n",
    "    print(f\"Max subjet features: {max_subjet_features}\")\n",
    "    print(f\"Max subjet length: {max_subjet_length}\")\n",
    "    \n",
    "    padded_subjets = []\n",
    "    padded_subjet_masks = []\n",
    "    padded_particle_masks = []\n",
    "    \n",
    "    for i, (s, sm, pm) in enumerate(zip(subjets, subjet_masks, particle_masks)):\n",
    "        pad_subjets = max_subjets - s.size(0)\n",
    "        pad_features = max_subjet_features - s.size(1)\n",
    "        pad_length = max_subjet_length - s.size(2)\n",
    "        \n",
    "        print(f\"Padding required - Subjets: {pad_subjets}, Features: {pad_features}, Length: {pad_length}\")\n",
    "\n",
    "        padded_s = F.pad(s, (0, pad_length, 0, pad_features, 0, pad_subjets), \"constant\", 0)\n",
    "        padded_subjets.append(padded_s)\n",
    "        \n",
    "        padded_sm = F.pad(sm, (0, pad_subjets), \"constant\", 0)\n",
    "        padded_subjet_masks.append(padded_sm)\n",
    "        \n",
    "        padded_pm = F.pad(pm, (0, pad_length, 0, pad_subjets), \"constant\", 0)\n",
    "        padded_particle_masks.append(padded_pm)\n",
    "        \n",
    "        print(f\"Padded subjets shape: {padded_s.shape}\")\n",
    "        print(f\"Padded subjet mask shape: {padded_sm.shape}\")\n",
    "        print(f\"Padded particle mask shape: {padded_pm.shape}\")\n",
    "    \n",
    "    subjets = torch.stack(padded_subjets)\n",
    "    subjet_masks = torch.stack(padded_subjet_masks)\n",
    "    particle_masks = torch.stack(padded_particle_masks)\n",
    "    \n",
    "    print(f\"\\nFinal stacked subjets shape: {subjets.shape}\")\n",
    "    print(f\"Final stacked subjet masks shape: {subjet_masks.shape}\")\n",
    "    print(f\"Final stacked particle masks shape: {particle_masks.shape}\")\n",
    "    \n",
    "    print(\"--- End of custom_collate_fn ---\\n\")\n",
    "    \n",
    "    return features, subjets, subjet_masks, particle_masks\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        print(\"Initializing Attention module\")\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Attention forward pass with input shape: {x.shape}\")\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        print(f\"Attention output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        print(\"Initializing MLP module\")\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"MLP forward pass with input shape: {x.shape}\")\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        print(f\"MLP output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing Block module\")\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = nn.Identity() if drop_path <= 0 else nn.Dropout(drop_path)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Block forward pass with input shape: {x.shape}\")\n",
    "        y = self.attn(self.norm1(x))\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        print(f\"Block output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class JetsTransformer(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim, depth, num_heads, mlp_ratio, qkv_bias=False, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing JetsTransformer module\")\n",
    "        self.num_features = num_features\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Adjust the input dimensions based on the new input shape\n",
    "        self.patch_embed = nn.Linear(num_features * 30, embed_dim)  # num_features * subjet_length\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 512, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate, norm_layer=norm_layer) for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"JetsTransformer forward pass with input shape: {x.shape}\")\n",
    "        B, N, C, L = x.shape\n",
    "        x = x.view(B, N, -1)  # Flatten last two dimensions to [B, N, C*L]\n",
    "        print(f\"Flattened input shape: {x.shape}\")\n",
    "        x = self.patch_embed(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        print(f\"JetsTransformer output shape: {x.shape}\")\n",
    "        return x.view(B, N, -1)  # Reshape back if necessary\n",
    "\n",
    "class JetsTransformerPredictor(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim, predictor_embed_dim, depth, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        print(\"Initializing JetsTransformerPredictor module\")\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate, norm_layer=norm_layer) for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, 8 * 30, bias=True)  # Match target dimensions\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        print(f\"JetsTransformerPredictor forward pass with input shape: {x.shape}\")\n",
    "        x = self.predictor_embed(x)\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_proj(x)\n",
    "        print(f\"JetsTransformerPredictor output shape: {x.shape}\")\n",
    "        return x.view(x.size(0), x.size(1), 8, 30)  # Reshape to match target_repr shape\n",
    "\n",
    "class JJEPA(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, depth, num_heads, mlp_ratio, dropout=0.1, use_predictor=True):\n",
    "        super(JJEPA, self).__init__()\n",
    "        print(\"Initializing JJEPA module\")\n",
    "        self.use_predictor = use_predictor\n",
    "        self.context_transformer = JetsTransformer(num_features=input_dim, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=dropout)\n",
    "        if self.use_predictor:\n",
    "            self.predictor_transformer = JetsTransformerPredictor(num_features=input_dim, embed_dim=embed_dim, predictor_embed_dim=embed_dim//2, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=dropout)\n",
    "\n",
    "\n",
    "        # Debug Statement - Dimension check\n",
    "        self.input_check = DimensionCheckLayer(\"Model Input\", 3)\n",
    "        self.context_check = DimensionCheckLayer(\"After Context Transformer\", 3)\n",
    "        self.predictor_check = DimensionCheckLayer(\"After Predictor\", 3)\n",
    "\n",
    "    def forward(self, context, target):\n",
    "        print(f\"JJEPA forward pass with context shape: {context.shape} and target shape: {target.shape}\")\n",
    "        context = context.to(next(self.parameters()).device)\n",
    "        target = target.to(next(self.parameters()).device)\n",
    "        \n",
    "        context_repr = self.context_transformer(context)\n",
    "        # Debug Statement\n",
    "        context_repr = self.context_check(context_repr)\n",
    "        if self.use_predictor:\n",
    "            pred_repr = self.predictor_transformer(context_repr, None, None)\n",
    "            pred_repr = self.predictor_check(pred_repr)\n",
    "            print(f\"JJEPA output - pred_repr shape: {pred_repr.shape}, context_repr shape: {context_repr.shape}, target shape: {target.shape}\")\n",
    "            return pred_repr, context_repr, target\n",
    "        \n",
    "        print(f\"JJEPA output - context_repr shape: {context_repr.shape}, target shape: {target.shape}\")\n",
    "        return context_repr, target\n",
    "\n",
    "def train_step(model, subjets, subjet_masks, particle_masks, optimizer, device, step):\n",
    "    print(f\"\\nStarting training step {step}\")\n",
    "    \n",
    "    # Debug Statement\n",
    "    check_processed_data(subjets)\n",
    "    \n",
    "    batch_size, num_subjets, num_features, subjet_length = subjets.size()\n",
    "    print(f\"Input shapes - Subjets: {subjets.shape}, Subjet masks: {subjet_masks.shape}, Particle masks: {particle_masks.shape}\")\n",
    "    \n",
    "    context_masks, target_masks = create_random_masks(batch_size, num_subjets, num_features, subjet_length)\n",
    "    print(f\"Context masks shape: {context_masks.shape}, Target masks shape: {target_masks.shape}\")\n",
    "    \n",
    "    context_masks = context_masks.to(device)\n",
    "    target_masks = target_masks.to(device)\n",
    "    subjet_masks = subjet_masks.to(device)\n",
    "    particle_masks = particle_masks.to(device)\n",
    "    \n",
    "    context_subjets = subjets * context_masks\n",
    "    target_subjets = subjets * target_masks\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Forwarding through model\")\n",
    "    pred_repr, context_repr, target_repr = model(context_subjets, target_subjets)\n",
    "    \n",
    "    print(f\"Predicted representation shape: {pred_repr.shape}\")\n",
    "    print(f\"Target representation shape: {target_repr.shape}\")\n",
    "    \n",
    "    combined_mask = target_masks.to(device) * subjet_masks.unsqueeze(-1).unsqueeze(-1).expand_as(target_masks).to(device)\n",
    "    \n",
    "    pred_repr = pred_repr.to(device)\n",
    "    target_repr = target_repr.to(device)\n",
    "    \n",
    "    print(\"Calculating loss\")\n",
    "    loss = F.mse_loss(pred_repr * combined_mask, target_repr * combined_mask)\n",
    "    print(f\"Calculated loss: {loss.item()}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print_jet_details(pred_repr[0].cpu(), \"Predicted\")\n",
    "        visualize_predictions_vs_ground_truth(subjets[0].cpu(), pred_repr[0].cpu(), title=f\"Ground Truth vs Predictions (Step {step})\")\n",
    "        print(f\"Context representation shape: {context_repr.shape}\")\n",
    "        print(f\"Target representation shape: {target_repr.shape}\")\n",
    "        \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def create_random_masks(batch_size, num_subjets, num_features, subjet_length, context_scale=0.7):\n",
    "    print(f\"Creating random masks with batch_size={batch_size}, num_subjets={num_subjets}\")\n",
    "    context_masks = []\n",
    "    target_masks = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        indices = torch.randperm(num_subjets)\n",
    "        context_size = int(num_subjets * context_scale)\n",
    "        context_indices = indices[:context_size]\n",
    "        target_indices = indices[context_size:]\n",
    "\n",
    "        context_mask = torch.zeros(num_subjets, num_features, subjet_length)\n",
    "        target_mask = torch.zeros(num_subjets, num_features, subjet_length)\n",
    "\n",
    "        context_mask[context_indices] = 1\n",
    "        target_mask[target_indices] = 1\n",
    "\n",
    "        context_masks.append(context_mask)\n",
    "        target_masks.append(target_mask)\n",
    "\n",
    "    return torch.stack(context_masks), torch.stack(target_masks)\n",
    "\n",
    "def normalize_features(features, feature_names, config, jet_type=\"Jets\"):\n",
    "    print(f\"Normalizing features with shape: {features.shape}\")\n",
    "    normalized_features = features.clone()\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        method = config[\"INPUTS\"][\"SEQUENTIAL\"][jet_type].get(feature_name, \"none\")\n",
    "        print(f\"Normalizing feature '{feature_name}' with method '{method}'\")\n",
    "\n",
    "        if method == \"normalize\":\n",
    "            mean = features[:, i].mean()\n",
    "            std = features[:, i].std()\n",
    "            std = std if std > 1e-8 else 1.0\n",
    "            normalized_features[:, i] = (features[:, i] - mean) / std\n",
    "        elif method == \"log_normalize\":\n",
    "            normalized_features[:, i] = torch.log1p(features[:, i])\n",
    "\n",
    "    print(f\"Normalized features shape: {normalized_features.shape}\")\n",
    "    return normalized_features\n",
    "\n",
    "def print_jet_details(jet, name):\n",
    "    print(f\"\\n{name} Jet Details:\")\n",
    "    print(f\"Shape: {jet.shape}\")\n",
    "    print(f\"Non-zero elements: {torch.count_nonzero(jet)}\")\n",
    "    print(\"\\nFirst few elements:\")\n",
    "    print(jet[0, :5, :5])\n",
    "\n",
    "def check_raw_data(subjets_data, jet_index=0):\n",
    "    print(f\"\\n--- Checking Raw Data for Jet {jet_index} ---\")\n",
    "    jet_subjets = subjets_data[jet_index]\n",
    "    print(f\"Number of subjets: {len(jet_subjets)}\")\n",
    "    print(f\"Subjet features: {list(jet_subjets[0]['features'].keys())}\")\n",
    "    print(f\"Sample subjet feature values: {jet_subjets[0]['features']}\")\n",
    "\n",
    "def check_processed_data(processed_subjets, batch_index=0):\n",
    "    print(f\"\\n--- Checking Processed Data for Batch Item {batch_index} ---\")\n",
    "    print(f\"Processed shape: {processed_subjets.shape}\")\n",
    "    if len(processed_subjets.shape) == 3:\n",
    "        num_subjets, num_features, subjet_length = processed_subjets.shape\n",
    "        print(f\"Number of subjets: {num_subjets}\")\n",
    "        print(f\"Number of features: {num_features}\")\n",
    "        print(f\"Subjet length: {subjet_length}\")\n",
    "        print(\"\\nFirst few values of each feature:\")\n",
    "        for i in range(num_features):\n",
    "            print(f\"Feature {i}: {processed_subjets[0, i, :5]}\")\n",
    "    else:\n",
    "        print(\"Unexpected shape for processed subjets\")\n",
    "\n",
    "def inspect_subjets(subjets, num_samples=5):\n",
    "    print(\"\\n--- Inspecting Subjets ---\")\n",
    "    for i in range(min(num_samples, len(subjets))):\n",
    "        subjet = subjets[i]\n",
    "        print(f\"\\nSubjet {i}:\")\n",
    "        print(f\"  pT: {subjet['features']['pT']:.2f}\")\n",
    "        print(f\"  eta: {subjet['features']['eta']:.2f}\")\n",
    "        print(f\"  phi: {subjet['features']['phi']:.2f}\")\n",
    "        print(f\"  num_ptcls: {subjet['features']['num_ptcls']}\")\n",
    "\n",
    "class DimensionCheckLayer(torch.nn.Module):\n",
    "    def __init__(self, name, expected_dims):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.expected_dims = expected_dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) != self.expected_dims:\n",
    "            print(f\"WARNING: {self.name} has {len(x.shape)} dimensions, expected {self.expected_dims}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def visualize_training_loss(train_losses):\n",
    "    print(\"Visualizing training loss\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting main program\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        \n",
    "    try:\n",
    "        print(\"Loading dataset\")\n",
    "        train_dataset = JetDataset(\"../data/val/val_20_30.h5\", subset_size=1000, config=config)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "    print(\"Creating DataLoader\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    print(\"Initializing model\")\n",
    "    model = JJEPA(input_dim=240, embed_dim=512, depth=12, num_heads=8, mlp_ratio=4.0, dropout=0.1).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.04)\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True, position=0)\n",
    "    \n",
    "        for step, (features, subjets, subjet_masks, particle_masks) in enumerate(train_loader):\n",
    "            features = features.to(device)\n",
    "            subjets = subjets.to(device)\n",
    "            subjet_masks = subjet_masks.to(device)\n",
    "            particle_masks = particle_masks.to(device)\n",
    "            \n",
    "            loss = train_step(model, subjets, subjet_masks, particle_masks, optimizer, device, step)\n",
    "            total_loss += loss\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss)\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"\\nEpoch {epoch+1}, Step {step}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        progress_bar.close()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    print(\"Visualizing training loss\")\n",
    "    visualize_training_loss(train_losses)\n",
    "\n",
    "    print(\"Saving model\")\n",
    "    torch.save(model.state_dict(), 'ijepa_model.pth')\n",
    "\n",
    "    print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c5f8f-f921-4bb9-99be-b6dea7aa7c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
